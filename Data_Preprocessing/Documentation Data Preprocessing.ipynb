{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing csv data\n",
    "- pandas.read_csv was used to open Data.csv(in same directory) as a DataFrame\n",
    "- then we divided the dataset into independent variables(X) and dependent variables(y) using iloc that separates the dataset using indexes (before the comma are line indexes and after column ones)\n",
    "- and finally transforming them into numpy arrays with \".values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------dataset--------------\n",
      "\n",
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print('--------------dataset--------------')\n",
    "print()\n",
    "print(dataset)\n",
    "print()\n",
    "print('-----------------------------------')\n",
    "print()\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------independent variables-------\n",
      "\n",
      "array([['France', 44.0, 72000.0],\n",
      "       ['Spain', 27.0, 48000.0],\n",
      "       ['Germany', 30.0, 54000.0],\n",
      "       ['Spain', 38.0, 61000.0],\n",
      "       ['Germany', 40.0, nan],\n",
      "       ['France', 35.0, 58000.0],\n",
      "       ['Spain', nan, 52000.0],\n",
      "       ['France', 48.0, 79000.0],\n",
      "       ['Germany', 50.0, 83000.0],\n",
      "       ['France', 37.0, 67000.0]], dtype=object)\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('-------independent variables-------')\n",
    "print()\n",
    "pprint(X)\n",
    "print()\n",
    "print('-----------------------------------')\n",
    "print()\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------dependent variables--------\n",
      "\n",
      "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
      "      dtype=object)\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('--------dependent variables--------')\n",
    "print()\n",
    "pprint(y)\n",
    "print()\n",
    "print('-----------------------------------')\n",
    "print()\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking Care of Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- first we have created an object from the SimpleImputer class, this objects receives which are the missing values that we want to replace (in this case NaN values) and the strategy we will use to replace the missing values (the mean between all other column values was chosen)\n",
    "- then we use the fit function with the lines and columns we want to transform as parameters (in this case all lines of all numerical columns), this function will find the missing data and calculate the mean number\n",
    "- last but not least we execute the imputer.transform function that will use the information acquired through the fit function to replace all missing data and we save the result in the right place of the X array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:])\n",
    "X[:, 1:] = imputer.transform(X[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([['France', 44.0, 72000.0],\n",
      "       ['Spain', 27.0, 48000.0],\n",
      "       ['Germany', 30.0, 54000.0],\n",
      "       ['Spain', 38.0, 61000.0],\n",
      "       ['Germany', 40.0, 63777.77777777778],\n",
      "       ['France', 35.0, 58000.0],\n",
      "       ['Spain', 38.77777777777778, 52000.0],\n",
      "       ['France', 48.0, 79000.0],\n",
      "       ['Germany', 50.0, 83000.0],\n",
      "       ['France', 37.0, 67000.0]], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "pprint(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking Care of Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nominal Categorical Data\n",
    "- at first we have created an object from the ColumnTransformer class\n",
    "    - this object will receive 2 parameters, transformers an remainder, remainder will say what to do with the columns that we have not specified (in this case passthrough, that will keep the columns and do nothing with them), transformers is a little more complex, it will receive a list of tuples, each tuple will specify an action we want to take in a certain group of columns, the first element will be the transformer name('encoder'), the second one what we will use to transform(OneHotEncoder()) and the third element will be the columns in which we want to execute this action (we could use a list or a slice)\n",
    "    - now talking more about the OneHotEncoder: It will create a column for each categorical value, and treat it somewhat like a boolean value. For exemple, we will have a column for Germany and in this column every line that had the \"Germany\" value will be filled in with 1 and every other line will be filled in with a 0\n",
    "\n",
    "- then we will fit and transform it at once, make it a numpy array and then save again inside X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer(transformers=[('encoder', \n",
    "                                                       OneHotEncoder(),\n",
    "                                                       [0])],\n",
    "                                       remainder='passthrough')\n",
    "X = np.array(column_transformer.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
      "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
      "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
      "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
      "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
      "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
      "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
      "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
      "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
      "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "pprint(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Categorical Data\n",
    "\n",
    "- this time we will use LabelEncoder that is much simpler then ColumnTransformer and OneHotEncoder, and we can use it since we have only two values ('Yes' and 'No'). The LabelEncoder will transform y in a column with zeros and ones and each number will represent one categorical value (just like a boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "pprint(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Training from Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for splitting the dataset into training and test set we will use the function train_test_split from the sklearn.model_selection library\n",
    "- this function will receive X, y and the size of the test or training set as parameters. We could also use the parameter random_state if we wanted to obtain the same result everytime we execute the code, this parameter will be like a seed\n",
    "- and it will return a list of arrays, in the order X_train, X_test, y_train, y_test\n",
    "- we have to separate the sets to avoid overfitting, that would happen if we trained and tested in the same set (the model would be great to predict in this set but we would not be sure about other sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1.0, 0.0, 0.0, 48.0, 79000.0],\n",
      "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
      "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
      "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
      "       [1.0, 0.0, 0.0, 44.0, 72000.0],\n",
      "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
      "       [1.0, 0.0, 0.0, 37.0, 67000.0],\n",
      "       [0.0, 0.0, 1.0, 27.0, 48000.0]], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "pprint(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
      "       [0.0, 0.0, 1.0, 38.0, 61000.0]], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "pprint(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 1, 1, 0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "pprint(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 0])\n"
     ]
    }
   ],
   "source": [
    "pprint(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying feature scaling in both Test and Training set\n",
    " - due to how feature scaling is done, we have to do feature scaling after separating the sets, otherwise some Training information would be inside the Test set and it cannot happen\n",
    " - we have two types of feature scaling, normalization and standardization. Normalization works when most of your features follow a normal distribution, while standardization works everytime, so we will use standardization this time\n",
    " - standardization will keep numbers between -3 and 3, while normalizarion keep numbers between 0 and 1\n",
    " - we don't need to use feature scaling in our dummy data (that we got from the categorical data) since it already is  0 or 1, and after using feature scalling we won't be abble to now which country is which\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization formula:\n",
    "\\begin{equation*}\n",
    "x_{norm} = \\frac{x - min(x)}{max(x) - min(x)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization formula:\n",
    "\\begin{equation*}\n",
    "x_{stand} = \\frac{x - mean(x)}{standard\\_deviation(x)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for the standardization we will use the StandardScaler class from the sklearn.preprocessing library\n",
    "- first of all we simply created the object with no parameters, since we will use the default ones\n",
    "- then we have used the fit_transform method in the part of the training set that we want to scale, this method will create the scaler with the mean and standard deviation and then apply it in the training set\n",
    "- since we already have created the scaler, and we want to  use the same metrics in the training and test set, we will just use the transform method in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "X_train[:, 3:] = standard_scaler.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = standard_scaler.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 1.1868742003402273 1.1808268575196743]\n",
      " [0.0 1.0 0.0 0.14632695620632938 -0.16029877547676347]\n",
      " [1.0 0.0 0.0 -0.5040150713773568 -0.669339161723587]\n",
      " [0.0 1.0 0.0 1.4470110113737018 1.533239432613629]\n",
      " [1.0 0.0 0.0 0.6666005782732783 0.5641048511052539]\n",
      " [0.0 1.0 0.0 -1.154357098961043 -1.0217517368175415]\n",
      " [1.0 0.0 0.0 -0.24387826034388232 0.12358913223781073]\n",
      " [0.0 0.0 1.0 -1.5445623155112547 -1.5503705994584733]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.01264553942523824 -1.1979580243645187]\n",
      " [0.0 0.0 1.0 -0.11380985482714508 -0.40502973040312107]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
